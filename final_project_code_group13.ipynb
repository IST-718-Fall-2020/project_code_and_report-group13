{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_project_code_group13.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"name":"projtct","notebookId":581745788654180},"cells":[{"cell_type":"markdown","metadata":{"id":"xD5xHBNqN6P5"},"source":["# **IST718 Group 13 Project Code**\r\n","- 50"]},{"cell_type":"markdown","metadata":{"id":"MAoHkRzmOBNL"},"source":["# Dataset Installing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbOg9XypMu3b","executionInfo":{"status":"ok","timestamp":1607758798843,"user_tz":300,"elapsed":31563,"user":{"displayName":"Vidushi Mishra","photoUrl":"","userId":"10528386041736211861"}},"outputId":"5d7bf93d-7f53-4ccf-c4fc-e517f593273e"},"source":["%%bash\n","# Do not change or modify this file\n","# Need to install pyspark\n","# if pyspark is already installed, will print a message indicating pyspark already isntalled\n","pip install pyspark\n","\n","# if [[ ! -f ./Telco-Customer-Churn.csv ]]; then \n","#    # download the data file from github and save it in this colab environment instance\n","#    wget https://raw.githubusercontent.com/YY-CHENG/IST/master/Telco_Customer_Churn.csv \n","# fi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","Collecting py4j==0.10.9\n","  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py): started\n","  Building wheel for pyspark (setup.py): finished with status 'done'\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=f45c55867bc751543d765216e30217d45567d7ea78058628ba80d8c2ab04b838\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CvFKoMbC6b8i"},"source":["from pyspark.sql.functions import col, when\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark.sql.types import FloatType, DoubleType\n","\n","from pyspark.ml.clustering import KMeans\n","from pyspark.ml import Pipeline, feature, clustering\n","from pyspark.ml.feature import StringIndexer, StringIndexerModel, OneHotEncoder, VectorAssembler, PCA, StandardScaler\n","from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC, GBTClassifier, LinearSVC\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, precision_recall_curve\n","import matplotlib.patches as mpatches\n","\n","%matplotlib inline\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fo_Ch0GNxT5"},"source":["spark = SparkSession \\\n","  .builder \\\n","  .master(\"local[*]\")\\\n","  .config(\"spark.memory.fraction\", 0.8) \\\n","  .config(\"spark.executor.memory\", \"12g\") \\\n","  .config(\"spark.driver.memory\", \"12g\")\\\n","  .config(\"spark.memory.offHeap.enabled\",'true')\\\n","  .config(\"spark.memory.offHeap.size\",\"12g\")\\\n","  .getOrCreate()\n","sc = spark.sparkContext\n","sqlContext = SQLContext(sc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2i_6HkU3NyLC"},"source":["#Telco_Customer_Churn_csv = '/content/drive/My Drive/ist718_data/data_group13/Telco_Customer_Churn.csv'\r\n","Telco_Customer_Churn_csv = 'Telco_Customer_Churn.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2fc5r0k64h6"},"source":["# added by grader\r\n","# install pydrive to load data\r\n","!pip install -U -q PyDrive\r\n","\r\n","from pydrive.auth import GoogleAuth\r\n","from pydrive.drive import GoogleDrive\r\n","from google.colab import auth\r\n","from oauth2client.client import GoogleCredentials\r\n","\r\n","auth.authenticate_user()\r\n","gauth = GoogleAuth()\r\n","gauth.credentials = GoogleCredentials.get_application_default()\r\n","drive = GoogleDrive(gauth)\r\n","\r\n","id = \"1byxR__P7iI4oydVTxlXEi0uZvOTTmTN0\"\r\n","file = drive.CreateFile({'id':id}) \r\n","file.GetContentFile('Telco_Customer_Churn.csv') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7DQd3PzNz9C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607759512371,"user_tz":300,"elapsed":17783,"user":{"displayName":"Vidushi Mishra","photoUrl":"","userId":"10528386041736211861"}},"outputId":"4678ab84-8523-49ec-f2ed-614cc481d45f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# You must use the definitions above to load your data.\n","#telco_churn_df = pd.read_csv(Telco_Customer_Churn_csv)\n","\n","telco_churn_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(Telco_Customer_Churn_csv)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4aQfPpUHQ2gy"},"source":["# Grid search switches\n","grid_search = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMHyKbUUdRe5"},"source":["# Dataset Cleaning"]},{"cell_type":"code","metadata":{"id":"RhGyPlhkYcTi"},"source":["#telco_churn_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('Telco_Customer_Churn.csv')\n","\n","# Data type inspection\n","telco_churn_df.printSchema()\n","\n","# Data inspection\n","telco_churn_df.toPandas().head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mf9rwABFYcTt"},"source":["# Data preprocessing\n","- Drop customerID\n","- Data type transform\n","- There are only 11 missing values in TotalCharges column. This values are actually a blank space in the csv file and are exclusive for customers with zero tenure. It's possible to concluded that they are missing due to the fact that the customer never paied anything to the company. We will impute this missing values with zero."]},{"cell_type":"code","metadata":{"id":"xRPH9eTRYcTt"},"source":["# Delete customerID and change datatypes \n","telco_churn_df = telco_churn_df.drop('customerID')\n","telco_churn_df = telco_churn_df.withColumn(\"tenure\",col(\"tenure\").cast(DoubleType()))\n","telco_churn_df = telco_churn_df.withColumn(\"MonthlyCharges\",col(\"MonthlyCharges\").cast(DoubleType()))\n","telco_churn_df = telco_churn_df.withColumn(\"TotalCharges\",col(\"TotalCharges\").cast(DoubleType()))\n","\n","# remove NA in TotalCharges\n","telco_churn_df = telco_churn_df.na.drop()\n","telco_churn_df = telco_churn_df.withColumn(\"TotalCharges\",col(\"TotalCharges\").cast(DoubleType()))\n","\n","cat_vars = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n","            'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', \n","            'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n","num_vars = ['tenure', 'MonthlyCharges', 'TotalCharges']\n","\n","# inspect dataset\n","telco_churn_df.toPandas().head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMvpuHj-PEcJ"},"source":["Statistic Inspections"]},{"cell_type":"code","metadata":{"id":"bpVQ1YUzYcT4"},"source":["telco_churn_df.select(num_vars).describe().toPandas()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zph5B_uIYcT5","scrolled":true},"source":["telco_churn_df.crosstab(\"gender\", \"churn\").show()\n","for col in cat_vars:\n","  display(telco_churn_df.groupBy(col).count().toPandas())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"roaea-zHYcT7"},"source":["# Visualization\n","- Churn summary\n","- Distribution\n","- Line Graphs for Numeric Variables\n","- Bar Plots for Categorical Variables\n","- Heatmap\n","- Pairplot"]},{"cell_type":"markdown","metadata":{"id":"AgVLaeE9YcT8"},"source":["## Numeric Variables"]},{"cell_type":"code","metadata":{"id":"uvzMeqSkYcT8"},"source":["telco_churn_df_pd = telco_churn_df.toPandas()\n","# check the traget variable\n","C = telco_churn_df_pd['Churn'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3,color =['#A1D9AD','#42A5CA'], alpha = 0.5)\n","C.set_ylabel('Number of Customers')\n","C.set_xlabel('Churn')\n","C.set_title('Number of Customers by Churn')\n","C.grid(False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eg2RT2aHYcUA"},"source":["# Tenure\n","T = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['tenure'].dropna(),label= 'Churn: No', color=\"Red\", shade = True)\n","T = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['tenure'].dropna(),label= 'Churn: Yes', color=\"Orange\", shade = True)\n","T.set_ylabel('Density')\n","T.set_xlabel('Tenure')\n","T.set_title('Distribution of tenure by churn')\n","T.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3OzD3AtYcUB"},"source":["# MonthlyCharges\n","MC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['MonthlyCharges'].dropna(), color= 'Red', label= 'Churn: No', shade = True)\n","MC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['MonthlyCharges'].dropna(), color= 'Orange', label= 'Churn: Yes', shade = True)\n","MC.set_ylabel('Density')\n","MC.set_xlabel('Monthly Charges')\n","MC.set_title('Distribution of monthly charges by churn')\n","MC.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgWRaufiYcUD"},"source":["# TotalCharges\n","TC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['TotalCharges'].dropna(), color= 'Red', label= 'Churn: No', shade = True)\n","TC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['TotalCharges'].dropna(), color= 'Orange', label= 'Churn: Yes', shade = True)\n","TC.set_ylabel('Density')\n","TC.set_xlabel('Total Charges')\n","TC.set_title('Distribution of total charges by churn')\n","TC.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQN_yawMYcUF"},"source":["plt.figure(figsize=(30, 10))\n","plt.subplot(2, 3, 1)\n","T = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['tenure'].dropna(),label= 'Churn: No', color='#A1D9AD',  shade = True)\n","T = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['tenure'].dropna(),label= 'Churn: Yes', color='#42A5CA', shade = True)\n","T.set_ylabel('Density')\n","T.set_xlabel('Tenure')\n","T.set_title('Distribution of tenure by churn')\n","T.legend()\n","\n","plt.subplot(2, 3, 2)\n","# MonthlyCharges\n","MC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['MonthlyCharges'].dropna(), color= '#A1D9AD', label= 'Churn: No', shade = True)\n","MC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['MonthlyCharges'].dropna(), color= '#42A5CA', label= 'Churn: Yes', shade = True)\n","MC.set_ylabel('Density')\n","MC.set_xlabel('Monthly Charges')\n","MC.set_title('Distribution of monthly charges by churn')\n","MC.legend()\n","\n","plt.subplot(2, 3, 3)\n","# TotalCharges\n","TC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'No']['TotalCharges'].dropna(), color= '#A1D9AD', label= 'Churn: No', shade = True)\n","TC = sns.kdeplot(telco_churn_df_pd[telco_churn_df_pd['Churn'] == 'Yes']['TotalCharges'].dropna(), color= '#42A5CA', label= 'Churn: Yes', shade = True)\n","TC.set_ylabel('Density')\n","TC.set_xlabel('Total Charges')\n","TC.set_title('Distribution of total charges by churn')\n","TC.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WEVocwoYcUJ"},"source":["import pandas as pd\n","plt.figure(figsize=(18, 10))\n","H = sns.heatmap(telco_churn_df_pd.\n","                apply(lambda x: pd.factorize(x)[0]).corr(), \n","                annot=True,\n","                fmt=\".2f\", \n","                xticklabels=telco_churn_df_pd.apply(lambda x: pd.factorize(x)[0]).corr().columns, \n","                yticklabels=telco_churn_df_pd.apply(lambda x: pd.factorize(x)[0]).corr().columns, \n","                linewidths=.1, cmap=\"Blues\")\n","H.set_ylabel('Attributes')\n","H.set_xlabel('Attributes')\n","H.set_title('Correlation heatmap')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tonBJg1yYcUK"},"source":["## Categorical Variables"]},{"cell_type":"code","metadata":{"id":"lhl7divGYcUL","scrolled":true},"source":["# plot categorical variables vs. churn\n","\n","for col in cat_vars:\n","\n","    plt.figure(figsize = (10, 5))\n","    ax = sns.countplot(x = col, hue = 'Churn', data = telco_churn_df_pd, palette = \"pastel\")\n","    ax.set_title(\"Churn Distribution across {}\".format(col), fontsize = 16)\n","    \n","    bars = ax.patches\n","    half = int(len(bars)/2)\n","    left_bars = bars[:half]\n","    right_bars = bars[half:]\n","\n","    for left, right in zip(left_bars, right_bars):\n","        height_l = left.get_height()\n","        height_r = right.get_height()\n","        total = height_l + height_r\n","\n","        ax.text(left.get_x() + left.get_width()/2., height_l + 10, '{0:.0%}'.format(height_l/total), ha=\"center\")\n","        ax.text(right.get_x() + right.get_width()/2., height_r + 10, '{0:.0%}'.format(height_r/total), ha=\"center\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98rjuAIpYcUM"},"source":["telco_churn_df_pd['PaymentMethod'] = telco_churn_df_pd['PaymentMethod'].replace(['Credit card (automatic)'],'Credit card')\n","telco_churn_df_pd['PaymentMethod'] = telco_churn_df_pd['PaymentMethod'].replace(['Bank transfer (automatic)'],'Bank transfer')\n","\n","sns.set(style = 'whitegrid', rc = {\"grid.linewidth\": 0.1})\n","sns.set_context(\"talk\", rc = {\"font.size\":12, \"axes.titlesize\":20, \"axes.labelsize\":15}) \n","\n","cat_vars_2 = ['Partner', 'Contract', 'PaymentMethod', 'OnlineSecurity']\n","\n","fig = plt.figure(figsize=(20, 11))\n","plt.subplots_adjust(hspace = 0.3)\n","\n","for col, i in zip(cat_vars_2, range(1,5)):\n","    \n","    ax = fig.add_subplot(2,2,i)\n","    sns.countplot(x = col, hue = 'Churn', data = telco_churn_df_pd, palette = ['#A1D9AD', '#42A5CA'])\n","    ax.set_title(\"Churn Distribution across {}\".format(col), fontsize = 20)\n","    \n","    bars = ax.patches\n","    half = int(len(bars)/2)\n","    left_bars = bars[:half]\n","    right_bars = bars[half:]\n","\n","    for left, right in zip(left_bars, right_bars):\n","        height_l = left.get_height()\n","        height_r = right.get_height()\n","        total = height_l + height_r\n","\n","        ax.text(left.get_x() + left.get_width()/2., height_l + 10, '{0:.0%}'.format(height_l/total), ha=\"center\", fontsize = 14)\n","        ax.text(right.get_x() + right.get_width()/2., height_r + 10, '{0:.0%}'.format(height_r/total), ha=\"center\", fontsize = 14)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpCMN4WFPwdJ"},"source":["# ML Models\n"]},{"cell_type":"markdown","metadata":{"id":"7PETaZT6qBqE"},"source":["## Data Transformation"]},{"cell_type":"code","metadata":{"id":"56ItGNqvRMln"},"source":["bi_var = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n","tri_var = list(set(cat_vars) - set(bi_var))\n","\n","# indexing binary variables: male = 1, yes = 1\n","indexers = [StringIndexer(inputCol=column, outputCol=column+\"_idx\", stringOrderType = 'alphabetAsc').fit(telco_churn_df) for column in bi_var]\n","pipeline = Pipeline(stages=indexers)\n","df_transform = pipeline.fit(telco_churn_df).transform(telco_churn_df)\n","\n","indexers = [StringIndexer(inputCol='Churn', outputCol=\"label\", stringOrderType = 'alphabetAsc')]\n","pipeline = Pipeline(stages=indexers)\n","df_transform = pipeline.fit(df_transform).transform(df_transform)\n","\n","\n","# get the distinct values of each categorical column\n","pm_list = ['Credit card (automatic)', 'Mailed check', 'Bank transfer (automatic)', 'Electronic check']\n","ml_list = ['Yes', 'No', 'No phone service']\n","con_list = ['Month-to-month', 'One year', 'Two year']\n","common_list = ['Yes', 'No', 'No internet service']\n","is_list = ['No', 'Fiber optic', 'DSL']\n","# To convert the three categorical data into ordinal numeric data, we need to define the StringIndexerModel pipeline respectively\n","stage_1 = StringIndexerModel.from_labels(common_list, inputCol=\"TechSupport\", outputCol=\"TechSupport_idx\")\n","stage_2 = StringIndexerModel.from_labels(pm_list, inputCol=\"PaymentMethod\", outputCol=\"PaymentMethod_idx\")\n","stage_3 = StringIndexerModel.from_labels(ml_list, inputCol=\"MultipleLines\", outputCol=\"MultipleLines_idx\")\n","stage_4 = StringIndexerModel.from_labels(common_list, inputCol=\"StreamingTV\", outputCol=\"StreamingTV_idx\")\n","stage_5 = StringIndexerModel.from_labels(common_list, inputCol=\"OnlineBackup\", outputCol=\"OnlineBackup_idx\")\n","stage_6 = StringIndexerModel.from_labels(common_list, inputCol=\"DeviceProtection\", outputCol=\"DeviceProtection_idx\")\n","stage_7 = StringIndexerModel.from_labels(common_list, inputCol=\"StreamingMovies\", outputCol=\"StreamingMovies_idx\")\n","stage_8 = StringIndexerModel.from_labels(con_list, inputCol=\"Contract\", outputCol=\"Contract_idx\")\n","stage_9 = StringIndexerModel.from_labels(is_list, inputCol=\"InternetService\", outputCol=\"InternetService_idx\")\n","stage_10 = StringIndexerModel.from_labels(common_list, inputCol=\"OnlineSecurity\", outputCol=\"OnlineSecurity_idx\")\n","\n","# combine and encapsulate all the transformation codes into one pipeline\n","feature_engineering_pipe = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5, \n","                                            stage_6, stage_7, stage_8, stage_9, stage_10])\n","result = feature_engineering_pipe.fit(df_transform).transform(df_transform)\n","\n","result = result.drop(*bi_var)\n","result = result.drop(*tri_var)\n","result = result.drop('Churn')\n","df_transform = result.toDF(*(c.replace('_idx', '') for c in result.columns))\n","\n","encoders = [OneHotEncoder(inputCol=column, outputCol=column+\"_encoded\").fit(df_transform) for column in tri_var]\n","pipeline = Pipeline(stages=encoders)\n","df_encode = pipeline.fit(df_transform).transform(df_transform)\n","\n","df_encode = df_encode.drop(*tri_var)\n","df_encode = df_encode.toDF(*(c.replace('_encoded', '') for c in df_encode.columns))\n","\n","featname_encode = ['tenure', 'MonthlyCharges', 'TotalCharges', 'gender_male', 'SeniorCitizen', 'Partner',\n","                   'Dependents', 'PhoneService', 'PaperlessBilling', 'OnlineSecurity_yes', 'OnlineSecurity_no',\n","                   'TechSupport_yes', 'TechSupport_no', 'Contract_month', 'Contract_1year',\n","                   'MultipleLines_yes', 'MultipleLines_no', 'InternetService_no', 'InternetService_fiber', \n","                   'OnlineBackup_yes', 'OnlineBackup_no', 'StreamingTV_yes', 'StreamingTV_no',\n","                   'StreamingMovies_yes', 'StreamingMovies_no', 'DeviceProtection_yes', 'DeviceProtection_no',\n","                   'PaymentMethod_credit', 'PaymentMethod_mail', 'PaymentMethod_bank']\n","\n","prepro_pipe = Pipeline(stages=[VectorAssembler(inputCols=df_encode.drop('label').columns, outputCol='vec_features'),\n","                               StandardScaler(withMean=True, withStd = True, inputCol='vec_features', outputCol='zfeatures')])\n","fitted = prepro_pipe.fit(df_encode)\n","fitted_df = fitted.transform(df_encode)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VLbf0TVaqkr_"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"kU8VJancpNVL"},"source":["### Imbalancing handling\n","Their is imbalance in the dataset. Thankfully, in the case of logistic regression we have a technique called “Class Weighing”. "]},{"cell_type":"code","metadata":{"id":"j9GgQe_ui-7h"},"source":["train, test = fitted_df.randomSplit([0.7, 0.3], )\n","evaluator = BinaryClassificationEvaluator()\n","\n","fitted_df_pd = fitted_df.toPandas()\n","BalancingRatio= len(fitted_df_pd[fitted_df_pd['label'] == 0])/len(fitted_df_pd['label'])\n","print('BalancingRatio = {}'.format(BalancingRatio))\n","\n","train = train.withColumn(\"classWeights\", when(train.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n","train.select(\"classWeights\").show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFFS2NPGpvK0"},"source":["### Grid Search"]},{"cell_type":"code","metadata":{"id":"v_eV-71yjFap"},"source":["lr = LogisticRegression(labelCol=\"label\", featuresCol=\"zfeatures\", weightCol=\"classWeights\")\n","\n","if grid_search:\n","    \n","    # generate the grid object, which iterates over different combinations of paramters\n","    lrparamGrid = (ParamGridBuilder()\\\n","                   .addGrid(lr.elasticNetParam, [0.5, 0.2, 0.1, 0.3])\\\n","                   .addGrid(lr.regParam, [0.01, 0.02, 0.1, 0.5])\\\n","                   .build())\n","\n","    # generate a 3-fold cross validation model\n","    lrcv = CrossValidator(estimator = lr,\n","                          estimatorParamMaps = lrparamGrid,\n","                          evaluator = evaluator,\n","                          numFolds = 3)\n","\n","    lr_cvModel = lrcv.fit(train)\n","\n","    print('elasticNetParam - ', lr_cvModel.bestModel.getOrDefault('elasticNetParam'))\n","    print('regParam - ', lr_cvModel.bestModel.getOrDefault('regParam'))\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KexGDPoUqaOk"},"source":["### Best Model"]},{"cell_type":"code","metadata":{"id":"KOFXqI4EzdRC"},"source":["# Best Model\n","lr_pipe = Pipeline(stages = [LogisticRegression(labelCol=\"label\", featuresCol=\"zfeatures\", weightCol=\"classWeights\")\\\n","                             .setRegParam(0.01)\\\n","                             .setElasticNetParam(0.5)])\n","lr_best = lr_pipe.fit(train)\n","\n","predictions = lr_best.transform(test)\n","\n","\n","# Feature importance\n","lr_feat = pd.DataFrame({'Feature': featname_encode,\n","                        'Coefficients': lr_best.stages[0].coefficients.toArray()}).sort_values('Coefficients', ascending = False) \n","                       \n","# Plotting\n","fig, ax = plt.subplots(figsize =(8, 10))  \n","ax.barh(lr_feat['Feature'], lr_feat['Coefficients']) \n","\n","# Remove axes splines \n","for s in ['top', 'bottom', 'left', 'right']: \n","    ax.spines[s].set_visible(False) \n","  \n","# Remove x, y Ticks \n","ax.xaxis.set_ticks_position('none') \n","ax.yaxis.set_ticks_position('none') \n","  \n","# Add padding between axes and labels \n","ax.xaxis.set_tick_params(pad = 5) \n","ax.yaxis.set_tick_params(pad = 10) \n","  \n","# Add x, y gridlines \n","ax.grid(b = True, color ='grey', \n","        linestyle ='-.', linewidth = 0.5, \n","        alpha = 0.2)\n","\n","ax.set_title('Logistic Regression Feature Coefficients', loc ='left', ) \n","\n","ax.set_xlabel(\"Coefficients\")\n","ax.set_ylabel(\"Features\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYCkajcJqJ8B"},"source":["### Result Evaluation"]},{"cell_type":"code","metadata":{"id":"q4xiiKowjqij"},"source":["def result_evaluate(predictions):\n","  preds_and_labels = predictions.select(['prediction','label']).orderBy('prediction')\n","  metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n","  confuse = metrics.confusionMatrix().toArray()\n","  print('Test Accuracy - ', metrics.accuracy)\n","  print('Test Recall - ', confuse[0][0]/(confuse[0][0] + confuse[1][0]))\n","  print('Test Precision - ', confuse[0][0]/(confuse[0][0] + confuse[0][1]))\n","  print('Test F1 - ', 2*confuse[0][0]/(2*confuse[0][0] + confuse[0][1] + confuse[1][0]))\n","  evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n","  print('Test Area Under ROC - ', evaluator.evaluate(predictions))\n","  evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n","  print('Test Area under PR - ', evaluator.evaluate(predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYQDkS4kgtSR"},"source":["result_evaluate(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLLzX4ScPwoh"},"source":["## SVM"]},{"cell_type":"markdown","metadata":{"id":"5vk2tWjn1m-L"},"source":["### Grid Search"]},{"cell_type":"code","metadata":{"id":"Cr6TJ_IC8zxp"},"source":["svm = LinearSVC(labelCol=\"label\", featuresCol=\"zfeatures\", weightCol=\"classWeights\")\n","\n","if grid_search: \n","    # generate the grid object, which iterates over different combinations of paramters\n","    SVMparamGrid = (ParamGridBuilder()\\\n","                    .addGrid(svm.threshold, [0.2, 0.5, 0.7])\\\n","                    .addGrid(svm.regParam, [0.5, 0.01, 0.1])\\\n","                    .build())\n","\n","    # generate a 3-fold cross validation model\n","    svm_cv = CrossValidator(estimator = svm,\n","                            estimatorParamMaps = SVMparamGrid,\n","                            evaluator = evaluator,\n","                            numFolds = 3)\n","\n","    svm_cvModel = svm_cv.fit(train)\n","\n","    print('threshold - ', svm_cvModel.bestModel.getOrDefault('threshold'))\n","    print('regParam - ', svm_cvModel.bestModel.getOrDefault('regParam'))\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuDSMNFW13Ho"},"source":["### Best Model"]},{"cell_type":"code","metadata":{"id":"HxepbYeC6BJJ"},"source":["# Best Model\n","svm_pipe = Pipeline(stages = [LinearSVC(labelCol=\"label\", featuresCol=\"zfeatures\", weightCol=\"classWeights\")\\\n","                              .setRegParam(0.01).setThreshold(0.2)])\n","svm_best = svm_pipe.fit(train)\n","\n","predictions = svm_best.transform(test)\n","\n","# Feature importance\n","svm_feat = pd.DataFrame({'Feature': featname_encode,\n","                        'Coefficients': svm_best.stages[0].coefficients.toArray()}).sort_values('Coefficients', ascending = False) \n","                       \n","# Plotting\n","fig, ax = plt.subplots(figsize =(8, 10))  \n","ax.barh(svm_feat['Feature'], svm_feat['Coefficients']) \n","\n","# Remove axes splines \n","for s in ['top', 'bottom', 'left', 'right']: \n","    ax.spines[s].set_visible(False) \n","  \n","# Remove x, y Ticks \n","ax.xaxis.set_ticks_position('none') \n","ax.yaxis.set_ticks_position('none') \n","  \n","# Add padding between axes and labels \n","ax.xaxis.set_tick_params(pad = 5) \n","ax.yaxis.set_tick_params(pad = 10) \n","  \n","# Add x, y gridlines \n","ax.grid(b = True, color ='grey', \n","        linestyle ='-.', linewidth = 0.5, \n","        alpha = 0.2)\n","\n","ax.set_title('Linear Support Vector Machine Feature Coefficients', loc ='left', ) \n","\n","ax.set_xlabel(\"Coefficients\")\n","ax.set_ylabel(\"Features\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4KuqQdt1rFK"},"source":["### Result Evaluation"]},{"cell_type":"code","metadata":{"id":"7pzjmgTF9WGb"},"source":["result_evaluate(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWEKoYy2qqAF"},"source":["## Random Forest"]},{"cell_type":"markdown","metadata":{"id":"DNNaCr0S5Kud"},"source":["### Grid Search"]},{"cell_type":"code","metadata":{"id":"Da1DOTRrqqTB"},"source":["\n","if grid_search:\n","    \n","    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"zfeatures\", weightCol=\"classWeights\")\n","\n","    paramGrid = (ParamGridBuilder()\\\n","                 .addGrid(rf.maxDepth, [10, 15, 20])\\\n","                 .addGrid(rf.numTrees, [10, 15, 20, 30])\\\n","                 .build())\n","\n","    rf_cv = CrossValidator(estimator=rf, \n","                           estimatorParamMaps=paramGrid, \n","                           evaluator=evaluator,\n","                           numFolds = 3)\n","\n","    rf_Model = rf_cv.fit(train)\n","\n","    print('numTrees - ', rf_Model.bestModel.getNumTrees)\n","    print('maxDepth - ', rf_Model.bestModel.getOrDefault('maxDepth'))\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pN15F6f0Qel5"},"source":["### Best Model"]},{"cell_type":"code","metadata":{"id":"AyWR0T_86xuj"},"source":["# Best Model\n","rf_pipe = Pipeline(stages = [RandomForestClassifier(labelCol = 'label', featuresCol = 'zfeatures', weightCol=\"classWeights\")\\\n","                             .setNumTrees(30)\\\n","                             .setMaxDepth(10)])\n","rf_best = rf_pipe.fit(train)\n","\n","predictions = rf_best.transform(test)\n","    \n","# Feature importance\n","rf_feat = pd.DataFrame({'Feature': featname_encode,\n","                        'Importance_score': rf_best.stages[0].featureImportances}).sort_values('Importance_score', ascending = False) \n","                       \n","# Plotting\n","fig, ax = plt.subplots(figsize =(8, 10))  \n","ax.barh(rf_feat['Feature'], rf_feat['Importance_score']) \n","\n","# Remove axes splines \n","for s in ['top', 'bottom', 'left', 'right']: \n","    ax.spines[s].set_visible(False) \n","    \n","# Remove x, y Ticks \n","ax.xaxis.set_ticks_position('none') \n","ax.yaxis.set_ticks_position('none') \n","  \n","# Add padding between axes and labels \n","ax.xaxis.set_tick_params(pad = 5) \n","ax.yaxis.set_tick_params(pad = 10) \n","  \n","# Add x, y gridlines \n","ax.grid(b = True, color ='grey', \n","        linestyle ='-.', linewidth = 0.5, \n","        alpha = 0.2)\n","\n","ax.set_title('Random Forest Feature Importances', loc ='left', ) \n","\n","ax.set_xlabel(\"Importance Scores\")\n","ax.set_ylabel(\"Features\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzF1B4TOR04O"},"source":["### Result Evaluation"]},{"cell_type":"code","metadata":{"id":"wGAncNeOCLwI"},"source":["result_evaluate(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqex0-zMqrA5"},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{"id":"NkCmE-S5RtAO"},"source":["### Grid Search"]},{"cell_type":"code","metadata":{"id":"FbKjQHkCqrZI"},"source":["# GBT tuning\n","\n","if grid_search:\n","    gbt = GBTClassifier(labelCol = 'label', featuresCol = 'zfeatures', weightCol=\"classWeights\").setMaxIter(50)\n","\n","    # generate the grid object, which iterates over different combinations of paramters\n","    gbt_paramGrid = (ParamGridBuilder()\\\n","                     .addGrid(gbt.maxDepth, [2, 4, 6, 8])\\\n","                     #.addGrid(gbt.maxIter, [50, 100, 150, 200])\\\n","                     .addGrid(gbt.maxBins, [20, 30, 50])\n","                     .build())\n","\n","    # generate a 3-fold cross validation model\n","    gbt_cv = CrossValidator(estimator = gbt,\n","                            estimatorParamMaps = gbt_paramGrid,\n","                            evaluator = evaluator,\n","                            numFolds = 3)\n","\n","    gbtModel = gbt_cv.fit(train)\n","\n","    print('maxBins - ', gbtModel.bestModel.getOrDefault('maxBins'))\n","    print('maxIter - ', gbtModel.bestModel.getOrDefault('maxIter'))\n","    print('maxDepth - ', gbtModel.bestModel.getOrDefault('maxDepth'))\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C9MVJ3KTY-w4"},"source":["### Best Model"]},{"cell_type":"code","metadata":{"id":"hfuDiTOlnNY9"},"source":["# Best model\n","gbt_pipe = Pipeline(stages = [GBTClassifier(labelCol = 'label', featuresCol = 'zfeatures', weightCol=\"classWeights\")\\\n","                             .setMaxDepth(2).setMaxIter(50).setMaxBins(30)])\n","gbt_best = gbt_pipe.fit(train)\n","\n","predictions = gbt_best.transform(test)\n","\n","# Feature importance\n","gbt_feat = pd.DataFrame({'Feature': featname_encode,\n","                         'Importance_score': gbt_best.stages[0].featureImportances}).sort_values('Importance_score', ascending = False) \n","\n","fig, ax = plt.subplots(figsize =(8, 10))  \n","ax.barh(gbt_feat['Feature'], gbt_feat['Importance_score']) \n","\n","# Remove axes splines \n","for s in ['top', 'bottom', 'left', 'right']: \n","    ax.spines[s].set_visible(False) \n","  \n","# Remove x, y Ticks \n","ax.xaxis.set_ticks_position('none') \n","ax.yaxis.set_ticks_position('none') \n","  \n","# Add padding between axes and labels \n","ax.xaxis.set_tick_params(pad = 5) \n","ax.yaxis.set_tick_params(pad = 10) \n","  \n","# Add x, y gridlines \n","ax.grid(b = True, color ='grey', \n","        linestyle ='-.', linewidth = 0.5, \n","        alpha = 0.2)\n","\n","ax.set_title('Gradient Boosting Feature Importances', loc ='left', ) \n","\n","ax.set_xlabel(\"Importance Scores\")\n","ax.set_ylabel(\"Features\")              "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ayAW98rYy3r"},"source":["### Result Evaluation"]},{"cell_type":"code","metadata":{"id":"oP1CI2z9SkF0"},"source":["result_evaluate(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMIP9CEXgrCC"},"source":["\n","preds = predictions.select('label', 'probability')\\\n","                   .rdd.map(lambda row: (float(row['probability'][1]), float(row['label']))).collect()\n","\n","y_score, y_true = zip(*preds)\n","fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)\n","roc_auc = auc(fpr, tpr)  \n","\n","# visualization\n","plt.figure()\n","plt.plot(fpr, tpr, label = 'ROC curve (area = %0.4f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic (ROC)')\n","plt.legend(loc = \"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmniNlDYlYrT"},"source":["\n","precision, recall, thresholds = precision_recall_curve(y_true, y_score, pos_label = 1)\n","pr_auc = auc(recall, precision)\n","\n","# visualization\n","plt.figure()\n","plt.plot(recall, precision, label = 'PR curve (area = %0.4f)' % pr_auc)\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.3, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve')\n","plt.legend(loc = \"lower left\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd83E5r8qpYR"},"source":["## K-Means"]},{"cell_type":"code","metadata":{"id":"wXtR690x7Yrc"},"source":["#PCA\n","pca = feature.PCA(k=10, inputCol='zfeatures', outputCol='scores')\n","pipe_pca = Pipeline(stages=[pca]).fit(train)\n","pipe_pca_df = pipe_pca.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3-_d5oN3kyn"},"source":["plt.figure(figsize = (10, 5))\n","explained_var = pipe_pca.stages[0].explainedVariance\n","plt.plot(np.arange(1, len(explained_var)+1), explained_var)\n","plt.title(\"Dataset Scree Plot\")\n","plt.xlabel(\"Principal Component\")\n","plt.ylabel(\"Proportion Variance Explained\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kReiRPFg3mqv"},"source":["cum_sum = np.cumsum(explained_var)\n","plt.figure(figsize = (10, 5))\n","plt.plot(np.arange(1, len(explained_var)+1), cum_sum)\n","plt.title(\"Dataset Cumulative Sum of Variance Explained\")\n","plt.xlabel(\"Cumulative Components\")\n","plt.ylabel(\"Cumulative Sum of Variance Explained\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20FSbY-8qprQ"},"source":["if grid_search:\n","  ls = []\n","\n","  for a in range(2,7):\n","    kmeans = clustering.KMeans(k=a, featuresCol='zfeatures', predictionCol='kmean-feat')\n","    model_km = Pipeline(stages = [pca,kmeans]).fit(train)\n","    predictions = model_km.transform(test)\n","    evaluator = ClusteringEvaluator().setFeaturesCol(\"scores\").setPredictionCol(\"kmean-feat\").setMetricName('silhouette')\n","    silhouette = evaluator.evaluate(predictions)\n","    ls.append(silhouette)\n","\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kf0RmRkGFWwG"},"source":["if grid_search:\n","  plot_df = pd.DataFrame(data = {\"k\": range(2,7), 'silhouette': ls})\n","\n","  plt.figure()\n","  plt.plot(range(2,7), ls)\n","  plt.title(\"K vs silhouette Plot\")\n","  plt.xlabel(\"K\")\n","  plt.ylabel(\"Silhouette\")\n","\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQRa3ndaTE6o"},"source":["### Best Model"]},{"cell_type":"code","metadata":{"id":"S5VjFlWi3zRU"},"source":["# Best Model\n","km_pipe = Pipeline(stages = [pca, KMeans(k = 3, predictionCol='kmean-feat', featuresCol = 'zfeatures')])\n","km_best = km_pipe.fit(train)\n","km_best_df = km_best.transform(test)\n","\n","X = np.array(km_best_df.select('scores').rdd.map(lambda x: x['scores']).collect())\n","\n","k = np.array(km_best_df.select('kmean-feat').rdd.map(lambda x: x['kmean-feat']).collect())\n","\n","label_c = np.array(km_best_df.select('OnlineSecurity').rdd.map(lambda x: x['OnlineSecurity']).collect())\n","\n","\n","fig, ax = plt.subplots()\n","fig.set_figheight(10)\n","fig.set_figwidth(10)\n","scatter = ax.scatter(X[:,0], X[:,1], c = k, cmap = 'Set3')\n","\n","legend1 = ax.legend(*scatter.legend_elements(),\n","                    loc=\"lower left\", title=\"Cluster\")\n","ax.add_artist(legend1)\n","\n","fig.suptitle(\"PC2 Scores vs. PC1 Scores\", y=.92)\n","ax.set_xlabel(\"PC1\")\n","ax.set_ylabel(\"PC2\")\n","\n","# for i, txt in enumerate(label_c):\n","#           ax.annotate(txt, (X[i,0], X[i,1]), fontsize = 12, alpha=0.5)\n","    \n","display(plt.show())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Syq2h_BJQ2g4"},"source":["compare_feat = pd.merge(rf_feat, gbt_feat, on = 'Feature', how = 'left')\n","compare_feat = pd.merge(compare_feat, lr_feat, on = 'Feature', how = 'left')\n","compare_feat = pd.merge(compare_feat, svm_feat, on = 'Feature', how = 'left')\n","compare_feat.columns = ['Feature', 'RF_importance_score', 'GBT_importance_score', 'LR_coefficient', 'SVM_coefficient']\n","compare_feat"],"execution_count":null,"outputs":[]}]}